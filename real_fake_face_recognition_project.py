# -*- coding: utf-8 -*-
"""Real fake face recognition project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sq15k0s462SSPdH6Rs0HV-MY3Myg4Ti7

Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import os

# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
from glob import glob
import seaborn as sns
from PIL import Image
import tensorflow as tf
import sklearn
from sklearn.model_selection import train_test_split
# %load_ext tensorboard
import datetime, os

"""Preprocessing Data"""

#obtaining images from Google Drive
from google.colab import drive
drive.mount("/content/drive/")

#function which converts images into numpy arrays
read = lambda imname: np.asarray(Image.open(imname).convert("RGB"))

#converting images
train_fake = "/content/drive/MyDrive/real_and_fake_face/training_fake"
train_real = "/content/drive/MyDrive/real_and_fake_face/training_real"
ims_fake = [read(os.path.join(train_fake, filename)) for filename in os.listdir(train_fake)]
X_train_fake = np.array(ims_fake,dtype='uint8')
ims_real = [read(os.path.join(train_real, filename)) for filename in os.listdir(train_real)]
X_train_real = np.array(ims_fake,dtype='uint8')

#joining the fake and real images
X = np.concatenate([X_train_fake,X_train_real])
#assigning y values for real and fake images, y = 0 is fake, y = 1 is real
y = []
for x in range(len(X_train_fake)):
  y.append(0)
for x in range(len(X_train_real)):
  y.append(1)
y = np.array(y)
#splitting data into training data and test data
X_train,X_test,y_train,y_test = train_test_split(X,y)

"""Creating the CNN model"""

#CNN model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(filters=10,kernel_size=50,strides=3,activation='tanh',padding='same',input_shape=X_train[0].shape))
model.add(tf.keras.layers.MaxPool2D(pool_size=(5,5),strides=(3,3)))
model.add(tf.keras.layers.Conv2D(filters=8,kernel_size=30,strides=1,activation='tanh'))
model.add(tf.keras.layers.MaxPool2D(pool_size=(4,4),strides=(2,2)))
model.add(tf.keras.layers.Conv2D(filters=6,kernel_size=15,strides=1,activation='tanh'))
model.add(tf.keras.layers.Conv2D(filters=5,kernel_size=3,strides=1,activation='tanh'))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(units=256,activation='tanh'))
model.add(tf.keras.layers.Dense(units=128,activation='tanh'))
model.add(tf.keras.layers.Dense(units=60,activation='tanh'))
model.add(tf.keras.layers.Dense(units=48,activation='tanh'))
model.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))
model.compile(optimizer='Adam',loss='MeanSquaredError',metrics=['accuracy'])

!pip install split-folders
import splitfolders
splitfolders.ratio('/content/drive/MyDrive/real_and_fake_face', output="/content/drive/MyDrive/real_and_fake_face/output", seed=1337, ratio=(.8, 0.1,0.1))

from tensorflow.keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)
training_set = train_datagen.flow_from_directory("/content/drive/MyDrive/real_and_fake_face/output/train",
                                                 target_size = (600, 600),
                                                 batch_size = 20,
                                                 class_mode = 'binary')
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/real_and_fake_face/output/test',
                                            target_size = (600, 600),
                                            batch_size = 20,
                                            class_mode = 'binary')
val_datagen = ImageDataGenerator(rescale = 1./255)
val_set = val_datagen.flow_from_directory('/content/drive/MyDrive/real_and_fake_face/output/val',
                                            target_size = (600, 600),
                                            batch_size = 20,
                                            class_mode = 'binary')
model.fit(training_set, steps_per_epoch= 10, epochs=10, validation_data= val_set, validation_steps= 10)
model.evaluate(test_set)

# Commented out IPython magic to ensure Python compatibility.
logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tb_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)
history = model.fit(training_set, steps_per_epoch= 10, epochs=20, validation_data= val_set, validation_steps= 10,callbacks=[tb_callback])
# %tensorboard --logdir logs